<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [mapstate](#mapstate)
- [sql方式](#sql%E6%96%B9%E5%BC%8F)
- [HyperLogLog](#hyperloglog)
- [bitmap的使用](#bitmap%E7%9A%84%E4%BD%BF%E7%94%A8)
- [编码实现split distinct aggregation功能](#%E7%BC%96%E7%A0%81%E5%AE%9E%E7%8E%B0split-distinct-aggregation%E5%8A%9F%E8%83%BD)
  - [Split Distinct Aggregation](#split-distinct-aggregation)
  - [使用代码方式实现](#%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%A0%81%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0)
  - [总结](#%E6%80%BB%E7%BB%93)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

##flink 去重的方式

### mapstate
https://mp.weixin.qq.com/s?__biz=MzU5MTc1NDUyOA==&mid=2247484069&idx=1&sn=5b2099a1c43c642a7a2342ff2f079850&chksm=fe2b66eac95ceffc5dcd8e5b1a8d091c22e91a50ff9bed50de43694538ff1d6860f5b5b21905&scene=21#wechat_redirect
### sql方式
 第一种
 ```
 select datatime,count(distinct devid) from pv group by datatime
 ```
核心依靠distinctAccumulator与CountAccumulator.DistinctAccumulator 内部包含一个map结构，key 表示的是distinct的字段，value表示重复的计数，CountAccumulator就是一个计数器的作用，这两部分都是作为动态生成聚合函数的中间结果accumulator,透过之前的聚合函数的分析可知中间结果是存储在状态里面的，也就是容错并且具有一致性语义的
其处理流程是：

1,将devId 添加到对应的DistinctAccumulator对象中，首先会判断map中是否存在该devId, 不存在则插入map中并且将对应value记1，并且返回True;存在则将对应的value+1更新到map中，并且返回False

2,只有当返回True时才会对CountAccumulator做累加1的操作,以此达到计数目的

第二种方式
```
select count(*),datatime from(

select distinct devId,datatime from pv ) a

group by datatime
```
内部是一个对devId,datatime 进行distinct的计算，在flink内部会转换为以devId,datatime进行分组的流并且进行聚合操作，在内部会动态生成一个聚合函数，该聚合函数createAccumulators方法生成的是一个Row(0) 的accumulator 对象，其accumulate方法是一个空实现，也就是该聚合函数每次聚合之后返回的结果都是Row(0),通过之前对sql中聚合函数的分析(可查看GroupAggProcessFunction函数源码)， 如果聚合函数处理前后得到的值相同那么可能会不发送该条结果也可能发送一条撤回一条新增的结果，但是其最终的效果是不会影响下游计算的，在这里我们简单理解为在处理相同的devId,datatime不会向下游发送数据即可,也就是每一对devId,datatime只会向下游发送一次数据；

外部就是一个简单的按照时间维度的计数计算，由于内部每一组devId,datatime 只会发送一次数据到外部，那么外部对应datatime维度的每一个devId都是唯一的一次计数，得到的结果就是我们需要的去重计数结果。

两种方式对比

这两种方式最终都能得到相同的结果，但是经过分析其在内部实现上差异还是比较大，第一种在分组上选择datatime ，内部使用的累加器DistinctAccumulator 每一个datatime都会与之对应一个对象，在该维度上所有的设备id, 都会存储在该累加器对象的map中，而第二种选择首先细化分组，使用datatime+devId分开存储，然后外部使用时间维度进行计数，简单归纳就是：
第一种: datatime->Value{devI1,devId2..}
第二种: datatime+devId->row(0)
聚合函数中accumulator 是存储在ValueState中的，第二种方式的key会比第一种方式数量上多很多，但是其ValueState占用空间却小很多，而在实际中我们通常会选择Rocksdb方式作为状态后端，rocksdb中value大小是有上限的，第一种方式很容易到达上限，那么使用第二种方式会更加合适；
这两种方式都是全量保存设备数据的，会消耗很大的存储空间，但是我们的计算通常是带有时间属性的，那么可以通过配置StreamQueryConfig设置状态ttl。

### HyperLogLog

HyperLogLog算法 也就是基数估计统计算法，预估一个集合中不同数据的个数，也就是我们常说的去重统计，在redis中也存在hyperloglog 类型的结构，能够使用12k的内存，允许误差在0.81%的情况下统计2^64个数据，在这种大数据量情况下能够减少存储空间的消耗，但是前提是允许存在一定的误差。关于HyperLogLog算法原理可以参考这篇文章：https://www.jianshu.com/p/55defda6dcd2里面做了详细的介绍，其算法实现在开源java流式计算库stream-lib提供了其具体实现代码，由于代码比较长就不贴出来(可以后台回复hll ,获取flink使用hll去重的完整代码)。
测试一下其使用效果，准备了97320不同数据：


public static void main(String[] args) throws Exception{

        String filePath = "000000_0";
        BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(filePath)));
    
        Set<String> values =new HashSet<>();
        HyperLogLog logLog=new HyperLogLog(0.01); //允许误差
    
        String line = "";
        while ((line = br.readLine()) != null) {
            String[] s = line.split(",");
            String uuid = s[0];
            values.add(uuid);
            logLog.offer(uuid);
        }
       
        long rs=logLog.cardinality();
    }
当误差值为0.01 时; rs为98228，需要内存大小int[1366] //内部数据结构
当误差值为0.001时；rs为97304 ，需要内存大小int[174763]
误差越小也就越来越接近其真实数据，但是在这个过程中需要的内存也就越来越大，这个取舍可根据实际情况决定。

在开发中更多希望通过sql方式来完成，那么就将hll与udaf结合起来使用，实现代码如下：

public class HLLDistinctFunction extends AggregateFunction<Long,HyperLogLog> {

    @Override public HyperLogLog createAccumulator() {
        return new HyperLogLog(0.001);
    }
    
    public void accumulate(HyperLogLog hll,String id){
      hll.offer(id);
    }
    
    @Override public Long getValue(HyperLogLog accumulator) {
        return accumulator.cardinality();
    }
}
定义的返回类型是long 也就是去重的结果，accumulator是一个HyperLogLog类型的结构。

测试：


case class AdData(id:Int,devId:String,datatime:Long)object Distinct1 {  def main(args: Array[String]): Unit = {
val env=StreamExecutionEnvironment.getExecutionEnvironment
val tabEnv=StreamTableEnvironment.create(env)
tabEnv.registerFunction("hllDistinct",new HLLDistinctFunction)
val kafkaConfig=new Properties()
kafkaConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9092")
kafkaConfig.put(ConsumerConfig.GROUP_ID_CONFIG,"test1")
val consumer=new FlinkKafkaConsumer[String]("topic1",new SimpleStringSchema,kafkaConfig)
consumer.setStartFromLatest()
val ds=env.addSource(consumer)
.map(x=>{
val s=x.split(",")
AdData(s(0).toInt,s(1),s(2).toLong)
})
tabEnv.registerDataStream("pv",ds)
val rs=tabEnv.sqlQuery(      """ select hllDistinct(devId) ,datatime
from pv group by datatime
""".stripMargin)
rs.writeToSink(new PaulRetractStreamTableSink)
env.execute()
}
}
准备测试数据


1,devId1,1577808000000
1,devId2,1577808000000
1,devId1,1577808000000
得到结果：


4> (true,1,1577808000000)
4> (false,1,1577808000000)
4> (true,2,1577808000000)
其基本使用介绍到这里，后续还将进一步优化。

在HyperLogLog去重实现中，如果要求误差在0.001以内，那么就需要1048576个int, 也就是会消耗4M的存储空间，但是在实际使用中有很多的维度的统计是达不到这个数据量，那么可以在这里做一个优化，优化方式是：初始HyperLogLog内部使用存储是一个set集合，当set大小达到了指定大小(1048576)就转换为HyperLogLog存储方式。这种方式可以有效减小内存消耗。

实现代码：
public class OptimizationHyperLogLog {
//hyperloglog结构
private HyperLogLog hyperLogLog;
//初始的一个set
private Set<Integer> set;

    private double rsd;
    
    //hyperloglog的桶个数，主要内存占用
    private int bucket;
    
    public OptimizationHyperLogLog(double rsd){
        this.rsd=rsd;
        this.bucket=1 << HyperLogLog.log2m(rsd);
        set=new HashSet<>();      
       }

//插入一条数据
public void offer(Object object){
final int x = MurmurHash.hash(object);
int currSize=set.size();
if(hyperLogLog==null && currSize+1>bucket){
//升级为hyperloglog
hyperLogLog=new HyperLogLog(rsd);
for(int d: set){
hyperLogLog.offerHashed(d);
}
set.clear();
}

        if(hyperLogLog!=null){
            hyperLogLog.offerHashed(x);
        }else {
            set.add(x);
        }
    }
    
    //获取大小
    public long cardinality() {
      if(hyperLogLog!=null) return hyperLogLog.cardinality();
      return set.size();
    }
}

初始化：入参同样是一个允许的误差范围值rsd，计算出hyperloglog需要桶的个数bucket，也就需要是int数组大小，并且初始化一个set集合hashset;

数据插入：使用与hyperloglog同样的方式将插入数据转hash, 判断当前集合的大小+1是否达到了bucket，不满足则直接添加到set中，满足则将set里面数据转移到hyperloglog对象中并且清空set, 后续数据将会被添加到hyperloglog中；

这种写法没有考虑并发情况，在实际使用情况中也不会存在并发问题。

### bitmap的使用
在前面提到的精确去重方案都是会保存全量的数据，但是这种方式是以牺牲存储为代价的，而hyperloglog方式虽然减少了存储但是损失了精度，那么如何能够做到精确去重又能不消耗太多的存储呢，这篇主要讲解如何使用bitmap做精确去重。

ID-mapping
在使用bitmap去重需要将去重的id转换为一串数字，但是我们去重的通常是一串包含字符的字符串例如设备ID，那么第一步需要将字符串转换为数字，首先可能想到对字符串做hash，但是hash是会存在概率冲突的，那么可以使用美团开源的leaf分布式唯一自增ID算法，也可以使用Twitter开源的snowflake分布式唯一ID雪花算法，我们选择了实现相对较为方便的snowflake算法(从网上找的)，代码如下：
public class SnowFlake {

    /**
     * 起始的时间戳
     */
    private final static long START_STMP = 1480166465631L;
    
    /**
     * 每一部分占用的位数
     */
    private final static long SEQUENCE_BIT = 12; //序列号占用的位数
    
    private final static long MACHINE_BIT = 5;   //机器标识占用的位数
    
    private final static long DATACENTER_BIT = 5;//数据中心占用的位数
    
    /**
     * 每一部分的最大值
     */
    private final static long MAX_DATACENTER_NUM = -1L ^ (-1L << DATACENTER_BIT);
    
    private final static long MAX_MACHINE_NUM = -1L ^ (-1L << MACHINE_BIT);
    
    private final static long MAX_SEQUENCE = -1L ^ (-1L << SEQUENCE_BIT);
    
    /**
     * 每一部分向左的位移
     */
    private final static long MACHINE_LEFT = SEQUENCE_BIT;
    
    private final static long DATACENTER_LEFT = SEQUENCE_BIT + MACHINE_BIT;
    
    private final static long TIMESTMP_LEFT = DATACENTER_LEFT + DATACENTER_BIT;
    
    private long datacenterId;  //数据中心
    
    private long machineId;     //机器标识
    
    private long sequence = 0L; //序列号
    
    private long lastStmp = -1L;//上一次时间戳
    
    public SnowFlake(long datacenterId, long machineId) {
        if (datacenterId > MAX_DATACENTER_NUM || datacenterId < 0) {
            throw new IllegalArgumentException("datacenterId can't be greater than MAX_DATACENTER_NUM or less than 0");
        }
        if (machineId > MAX_MACHINE_NUM || machineId < 0) {
            throw new IllegalArgumentException("machineId can't be greater than MAX_MACHINE_NUM or less than 0");
        }
        this.datacenterId = datacenterId;
        this.machineId = machineId;
    }
    
    /**
     * 产生下一个ID
     *
     * @return
     */
    public synchronized long nextId() {
        long currStmp = getNewstmp();
        if (currStmp < lastStmp) {
            throw new RuntimeException("Clock moved backwards.  Refusing to generate id");
        }
    
        if (currStmp == lastStmp) {
            //相同毫秒内，序列号自增
            sequence = (sequence + 1) & MAX_SEQUENCE;
            //同一毫秒的序列数已经达到最大
            if (sequence == 0L) {
                currStmp = getNextMill();
            }
        } else {
            //不同毫秒内，序列号置为0
            sequence = 0L;
        }
    
        lastStmp = currStmp;
    
        return (currStmp - START_STMP) << TIMESTMP_LEFT //时间戳部分
                | datacenterId << DATACENTER_LEFT       //数据中心部分
                | machineId << MACHINE_LEFT             //机器标识部分
                | sequence;                             //序列号部分
    }
    
    private long getNextMill() {
        long mill = getNewstmp();
        while (mill <= lastStmp) {
            mill = getNewstmp();
        }
        return mill;
    }
    
    private long getNewstmp() {
        return System.currentTimeMillis();
    }


}

snowflake算法的实现是与机器码以及时间有关的，为了保证其高可用做了两个机器码不同的对外提供的服务。那么整个转换流程如下图：
图片

首先会从Hbase中查询是否有UID对应的ID，如果有则直接获取，如果没有则会调用ID-Mapping服务，然后将其对应关系存储到Hbase中，最后返回ID至下游处理。

UDF化
为了方便提供业务方使用，同样需要将其封装成为UDF, 由于snowflake算法得到的是一个长整型，因此选择了Roaring64NavgabelMap作为存储对象，由于去重是按照维度来计算，所以使用UDAF，首先定义一个accumulator:
public class PreciseAccumulator{

    private Roaring64NavigableMap bitmap;
    
    public PreciseAccumulator(){
        bitmap=new Roaring64NavigableMap();
    }
    
    public void add(long id){
        bitmap.addLong(id);
    }
    
    public long getCardinality(){
        return bitmap.getLongCardinality();
    }
}

udaf实现
public class PreciseDistinct extends AggregateFunction<Long, PreciseAccumulator> {

    @Override public PreciseAccumulator createAccumulator() {
        return new PreciseAccumulator();
    }
    
    public void accumulate(PreciseAccumulator accumulator,long id){
        accumulator.add(id);
    }
    
    @Override public Long getValue(PreciseAccumulator accumulator) {
        return accumulator.getCardinality();
    }
}
那么在实际使用中只需要注册udaf即可。

### 编码实现split distinct aggregation功能 
https://mp.weixin.qq.com/s?__biz=MzU5MTc1NDUyOA==&mid=2247484501&idx=1&sn=e34401a2e61bf89d25f38f30c807ff89&chksm=fe2b601ac95ce90cdba56794eb65cc2ece99791d4339f0528ea47b45a03fc152d1edb39df66f&scene=21#wechat_redirect


去重指标作为业务分析里面的一个重要指标，不管是在OLAP存储引擎还是计算引擎都对其实现做了大量工作，在面对不同的数据量、指标精确性要求，都有不同的实现方式，但是总体都逃脱不了硬算、两阶段方式、bitmap、hll等这些实现。本文将分析Split Distinct Aggregation实现原理与使用代码方式实现其功能。

#### Split Distinct Aggregation 

如果要使用Sql去实现一个去重功能，通常会这样实现：

```
SELECT day, COUNT(DISTINCT user_id) FROM T  GROUP BY day --sql1
```

或者

```
select day,count(*) from(     select distinct user_id,day from T ) agroup by day     --sql2
```

在之前的去重系列中SQL方式去重中也对这两种实现方式进行了分析，但是这两种方式都未解决计算热点问题，例如当某一个day 对应的devId 特别大的情况下，那么计算压力都会到该day所在的task，使这个task成为任务的性能瓶颈。

Split Distinct Aggregation是从Flink-1.9版本开始提供的一个对去重的优化功能，该功能必须在Blink planner下并且配置:

```
val tEnv: TableEnvironment = ...tEnv.getConfig.getConfiguration .setString("table.optimizer.distinct-agg.split.enabled", "true")
```

那么sql1 在其内部会转换为

```
SELECT day, SUM(cnt)FROM (    SELECT day, COUNT(DISTINCT user_id) as cnt    FROM T    GROUP BY day, MOD(HASH_CODE(user_id), 1024))GROUP BY day
```

MOD(HASH_CODE(user_id), 1024) 表示对取user_id的hashCode然后对1024取余，也就是将user_id划分到1024个桶里面去，那么里层通过对day与桶编号进行去重(cnt)外层只需要对cnt执行sum操作即可，因为分桶操作限制了相同的user_id 一定会在相同的桶里面，执行效果图如下：

![图片](640-20220216174636226)

我们也通过tabEnv.explain方式打印执行计划验证一下是否是真的这样执行：

```
Stage 5 : Operator                                          content : Calc(select=[status, devId, (HASH_CODE(devId) MOD 1024) AS $f2])
Stage 7 : Operator                                        content : GroupAggregate(groupBy=[status, $f2], partialFinalType=[PARTIAL], select=[status, $f2, COUNT(DISTINCT devId) AS $f2_0])ship_strategy : HASH
Stage 9 : Operator                                         content : GroupAggregate(groupBy=[status], partialFinalType=[FINAL], select=[status, $SUM0_RETRACT($f2_0) AS $f1])ship_strategy : HAS
```

Stage 5 中执行分桶操作，Stage 7分桶之后去重操作，Stage 9 最终的sum操作。

#### 使用代码方式实现  


在去重系列中实现了使用MapState去重方式，仍然在此基础上来完成Split Distinct Aggregation功能，其业务场景是实时计算广告位访客数，流量数据id(广告位ID)、devId(访问ID)、time(访问时间)，实现思路：

•首先通过对id、设备id分桶编号、小时级别时间分组，使用一个ProcessFunction计算分桶后的去重数(与MapState方式相同)•然后通过对id、小时级别时间分组，使用另一个ProcessFunction做sum操作，但是这里面需要注意的一个问题是对于相同id与时间其数据可能会来源于上游不同的task，而上游的每个task的数据都会以全量一直往下发送，如果直接做累加操作会导致重复计算，因此得实现一个类似于sql中retract撤回机制(可参考[Flink SQL中可撤回机制解密](http://mp.weixin.qq.com/s?__biz=MzU5MTc1NDUyOA==&mid=2247484023&idx=1&sn=48d4b48e9a906126c54b085946ffe350&chksm=fe2b6638c95cef2e7351d1327c9f83499d1dfa5d821e3b9478d2e4344b22d9d3245b09e810e2&scene=21#wechat_redirect))，也就是上一个ProcessFunction每发送一条数据都需要先将之前的数据发送一份表示其为撤回。



接下来看具体的代码实现，数据结构：

```
--流量数据case class AdData(id:Int,devId:String,time:Long)  --第一次keyBy数据case class AdKey1(id:Int,time:Long,bucketCode:Int) --第二次keyBy数据  case class AdKey2(id:Int,time:Long)
```

去重实现Distinct1ProcessFunction:


```
class Distinct1ProcessFunction extends KeyedProcessFunction[AdKey1, AdData, Tuple2[Boolean, Tuple3[Int, Long, Long]]] {  var devIdState: MapState[String, Int] = _  var devIdStateDesc: MapStateDescriptor[String, Int] = _  var countState: ValueState[Long] = _  var countStateDesc: ValueStateDescriptor[Long] = _
  override def open(parameters: Configuration): Unit = {
    devIdStateDesc = new MapStateDescriptor[String, Int]("devIdState", TypeInformation.of(classOf[String]), TypeInformation.of(classOf[Int]))    devIdState = getRuntimeContext.getMapState(devIdStateDesc)    countStateDesc = new ValueStateDescriptor[Long]("countState", TypeInformation.of(classOf[Long]))    countState = getRuntimeContext.getState(countStateDesc)  }  override def processElement(value: AdData, ctx: KeyedProcessFunction[AdKey1, AdData, Tuple2[Boolean, Tuple3[Int, Long, Long]]]#Context, out: Collector[Tuple2[Boolean, Tuple3[Int, Long, Long]]]): Unit = {
    val devId = value.devId    devIdState.get(devId) match {      case 1 => {        //表示已经存在      }      case _ => {        //表示不存在        devIdState.put(devId, 1)        val c = countState.value()        val currV = c + 1        countState.update(currV)
        if (currV > 1) {          --认为大于1的需要执行撤回          out.collect(Tuple2.apply(false, Tuple3.apply(ctx.getCurrentKey.id, ctx.getCurrentKey.time, c)))          out.collect(Tuple2.apply(true, Tuple3.apply(ctx.getCurrentKey.id, ctx.getCurrentKey.time, currV)))        } else {          out.collect(Tuple2.apply(true, Tuple3.apply(ctx.getCurrentKey.id, ctx.getCurrentKey.time, currV)))        }      }    }  }}
```

撤回实现同样使用boolean标识，false表示为撤回数据，true表示正常insert的数据。

聚合实现Distinct2ProcessFunction：


```
class Distinct2ProcessFunction extends KeyedProcessFunction[Tuple2[Int, Long], Tuple2[Boolean, Tuple3[Int, Long, Long]], Void] {  var cntState: ValueState[Long] = _  var cntStateDesc: ValueStateDescriptor[Long] = _
  override def open(parameters: Configuration): Unit = {    cntStateDesc = new ValueStateDescriptor[Long]("distinctValue", TypeInformation.of(classOf[Long]))    cntState = getRuntimeContext.getState(cntStateDesc)  }
  override def processElement(value: (Boolean, (Int, Long, Long)), ctx: KeyedProcessFunction[(Int, Long), (Boolean, (Int, Long, Long)), Void]#Context, out: Collector[Void]): Unit = {    val currV = cntState.value()    value._1 match {      case true => {        cntState.update(currV + value._2._3        println(ctx.getCurrentKey + ":" + cntState.value())      }      case false => {        --撤回操作        cntState.update(currV - value._2._3)        println(ctx.getCurrentKey + ":" + cntState.value())      }    }  }}
```

重点在于如果收到编码为false 的数据，那么需要从当前计数里面减掉撤回的计数值。

主流程：


```
val env = StreamExecutionEnvironment.getExecutionEnvironment
val kafkaConfig = new Properties()kafkaConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092")kafkaConfig.put(ConsumerConfig.GROUP_ID_CONFIG, "test1")val consumer = new FlinkKafkaConsumer[String]("topic1", new SimpleStringSchema, kafkaConfig)val ds = env.addSource(consumer)            .map(x => {                val s = x.split(",")                AdData(s(0).toInt, s(1), s(2).toLong)          }).keyBy(x => {               val endTime = TimeWindow.getWindowStartWithOffset(x.time, 0,               Time.hours(1).toMilliseconds) + Time.hours(1).toMilliseconds               AdKey1(x.id, endTime, x.devId.hashCode % 3)          }).process(new Distinct1ProcessFunction)            .keyBy(x => {                Tuple2.apply(x._2._1, x._2._2)          }).process(new Distinct2ProcessFunction)env.execute()
```



#### 总结

Split Distinct Aggregation是去重计算在数据倾斜的情况下的优化的一种思路，类似于两阶段聚合，第一阶段执行打散操作，第二阶段执行累加操作，这是一种通用的优化思路，而对于使用代码方式实现其重点在于第一阶段到第二阶段的撤回思路避免数据的重复计算。
